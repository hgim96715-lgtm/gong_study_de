{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d0e3567-b565-4ab7-870b-4018a1d962c9",
   "metadata": {},
   "source": [
    "## Hint 실습해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc5a934e-d6a5-4f41-bc0d-7ad5832951f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import (\n",
    "    Row,\n",
    "    SparkSession)\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f491d6e-d723-4e4f-ae31-7a5e15cd403c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/02/02 11:03:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark=(\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"spark-hint\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    #  자동 Broadcast 끄기\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "    # AQE 끄기\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ea63ca5-a512-42a3-b7c5-8568ffee0933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(user_id=0, amount=97.68326426258781),\n",
       " Row(user_id=1, amount=25.12428175168362)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 연습용 데이터 \n",
    "\n",
    "fact_df=(\n",
    "    spark\n",
    "    .range(0,10_000_000)\n",
    "    .withColumnRenamed(\"id\", \"user_id\")\n",
    "    .withColumn(\"amount\",F.rand()*100)\n",
    ")\n",
    "fact_df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d15ae3b9-f726-43c5-a6a6-d33ba9c08faa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(user_id=0, country='US'), Row(user_id=1, country='US')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 작은 테이블 연습용\n",
    "dim_df=(\n",
    "    spark\n",
    "    .range(0,100)\n",
    "    .withColumnRenamed(\"id\", \"user_id\")\n",
    "    .withColumn(\"country\",F.lit(\"US\"))\n",
    ")\n",
    "dim_df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f01a4fc4-b648-4fa9-a395-4911ff7538aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 힌트 없이 Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0786a318-e592-4e80-834f-2d5ac8f285aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (11)\n",
      "+- Project (10)\n",
      "   +- SortMergeJoin Inner (9)\n",
      "      :- Sort (4)\n",
      "      :  +- Exchange (3)\n",
      "      :     +- Project (2)\n",
      "      :        +- Range (1)\n",
      "      +- Sort (8)\n",
      "         +- Exchange (7)\n",
      "            +- Project (6)\n",
      "               +- Range (5)\n",
      "\n",
      "\n",
      "(1) Range\n",
      "Output [1]: [id#0L]\n",
      "Arguments: Range (0, 10000000, step=1, splits=Some(10))\n",
      "\n",
      "(2) Project\n",
      "Output [2]: [id#0L AS user_id#2L, (rand(4581568525187603755) * 100.0) AS amount#4]\n",
      "Input [1]: [id#0L]\n",
      "\n",
      "(3) Exchange\n",
      "Input [2]: [user_id#2L, amount#4]\n",
      "Arguments: hashpartitioning(user_id#2L, 200), ENSURE_REQUIREMENTS, [plan_id=49]\n",
      "\n",
      "(4) Sort\n",
      "Input [2]: [user_id#2L, amount#4]\n",
      "Arguments: [user_id#2L ASC NULLS FIRST], false, 0\n",
      "\n",
      "(5) Range\n",
      "Output [1]: [id#7L]\n",
      "Arguments: Range (0, 100, step=1, splits=Some(10))\n",
      "\n",
      "(6) Project\n",
      "Output [1]: [id#7L AS user_id#9L]\n",
      "Input [1]: [id#7L]\n",
      "\n",
      "(7) Exchange\n",
      "Input [1]: [user_id#9L]\n",
      "Arguments: hashpartitioning(user_id#9L, 200), ENSURE_REQUIREMENTS, [plan_id=50]\n",
      "\n",
      "(8) Sort\n",
      "Input [1]: [user_id#9L]\n",
      "Arguments: [user_id#9L ASC NULLS FIRST], false, 0\n",
      "\n",
      "(9) SortMergeJoin\n",
      "Left keys [1]: [user_id#2L]\n",
      "Right keys [1]: [user_id#9L]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(10) Project\n",
      "Output [3]: [user_id#2L, amount#4, US AS country#11]\n",
      "Input [3]: [user_id#2L, amount#4, user_id#9L]\n",
      "\n",
      "(11) AdaptiveSparkPlan\n",
      "Output [3]: [user_id#2L, amount#4, country#11]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join_no_hint=fact_df.join(dim_df,\"user_id\",\"inner\")\n",
    "join_no_hint.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4a9c514-ec3d-48df-b9e5-3b8a94b0e6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SortMergeJoin:  Spark가 Broadcast를 안 쓰고, 양쪽 데이터를 정렬 + 셔플해서 조인함\n",
    "# Exchange : 네트워크 셔플 발생 (파티션 재분배)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8778f70a-fec9-46b7-9faf-95be7046be0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 힌트 강제 \n",
    "# 큰 테이블.join(작은 테이블.hint(\"broadcast\"), 조건)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c571a28-da7d-4c26-9243-700c592d82d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (8)\n",
      "+- Project (7)\n",
      "   +- BroadcastHashJoin Inner BuildRight (6)\n",
      "      :- Project (2)\n",
      "      :  +- Range (1)\n",
      "      +- BroadcastExchange (5)\n",
      "         +- Project (4)\n",
      "            +- Range (3)\n",
      "\n",
      "\n",
      "(1) Range\n",
      "Output [1]: [id#0L]\n",
      "Arguments: Range (0, 10000000, step=1, splits=Some(10))\n",
      "\n",
      "(2) Project\n",
      "Output [2]: [id#0L AS user_id#2L, (rand(4581568525187603755) * 100.0) AS amount#4]\n",
      "Input [1]: [id#0L]\n",
      "\n",
      "(3) Range\n",
      "Output [1]: [id#7L]\n",
      "Arguments: Range (0, 100, step=1, splits=Some(10))\n",
      "\n",
      "(4) Project\n",
      "Output [1]: [id#7L AS user_id#9L]\n",
      "Input [1]: [id#7L]\n",
      "\n",
      "(5) BroadcastExchange\n",
      "Input [1]: [user_id#9L]\n",
      "Arguments: HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=79]\n",
      "\n",
      "(6) BroadcastHashJoin\n",
      "Left keys [1]: [user_id#2L]\n",
      "Right keys [1]: [user_id#9L]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(7) Project\n",
      "Output [3]: [user_id#2L, amount#4, US AS country#11]\n",
      "Input [3]: [user_id#2L, amount#4, user_id#9L]\n",
      "\n",
      "(8) AdaptiveSparkPlan\n",
      "Output [3]: [user_id#2L, amount#4, country#11]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join_broadcast=(\n",
    "    fact_df\n",
    "    .join(dim_df.hint(\"broadcast\"),\"user_id\",\"inner\")\n",
    ")\n",
    "join_broadcast.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4557c019-ea01-4e65-9002-4e47981dd827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 힌트 강제 성공 >  BroadcastHashJoin,BroadcastExchange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc211130-6fa7-4f0f-9897-054a6fce5989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge 강제 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ee7f9fe-0ae9-414e-b1ac-4c0198a7bc86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan (11)\n",
      "+- Project (10)\n",
      "   +- SortMergeJoin Inner (9)\n",
      "      :- Sort (4)\n",
      "      :  +- Exchange (3)\n",
      "      :     +- Project (2)\n",
      "      :        +- Range (1)\n",
      "      +- Sort (8)\n",
      "         +- Exchange (7)\n",
      "            +- Project (6)\n",
      "               +- Range (5)\n",
      "\n",
      "\n",
      "(1) Range\n",
      "Output [1]: [id#0L]\n",
      "Arguments: Range (0, 10000000, step=1, splits=Some(10))\n",
      "\n",
      "(2) Project\n",
      "Output [2]: [id#0L AS user_id#2L, (rand(4581568525187603755) * 100.0) AS amount#4]\n",
      "Input [1]: [id#0L]\n",
      "\n",
      "(3) Exchange\n",
      "Input [2]: [user_id#2L, amount#4]\n",
      "Arguments: hashpartitioning(user_id#2L, 200), ENSURE_REQUIREMENTS, [plan_id=107]\n",
      "\n",
      "(4) Sort\n",
      "Input [2]: [user_id#2L, amount#4]\n",
      "Arguments: [user_id#2L ASC NULLS FIRST], false, 0\n",
      "\n",
      "(5) Range\n",
      "Output [1]: [id#7L]\n",
      "Arguments: Range (0, 100, step=1, splits=Some(10))\n",
      "\n",
      "(6) Project\n",
      "Output [1]: [id#7L AS user_id#9L]\n",
      "Input [1]: [id#7L]\n",
      "\n",
      "(7) Exchange\n",
      "Input [1]: [user_id#9L]\n",
      "Arguments: hashpartitioning(user_id#9L, 200), ENSURE_REQUIREMENTS, [plan_id=108]\n",
      "\n",
      "(8) Sort\n",
      "Input [1]: [user_id#9L]\n",
      "Arguments: [user_id#9L ASC NULLS FIRST], false, 0\n",
      "\n",
      "(9) SortMergeJoin\n",
      "Left keys [1]: [user_id#2L]\n",
      "Right keys [1]: [user_id#9L]\n",
      "Join type: Inner\n",
      "Join condition: None\n",
      "\n",
      "(10) Project\n",
      "Output [3]: [user_id#2L, amount#4, US AS country#11]\n",
      "Input [3]: [user_id#2L, amount#4, user_id#9L]\n",
      "\n",
      "(11) AdaptiveSparkPlan\n",
      "Output [3]: [user_id#2L, amount#4, country#11]\n",
      "Arguments: isFinalPlan=false\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join_merge = (\n",
    "    fact_df\n",
    "    .join(dim_df.hint(\"merge\"), \"user_id\", \"inner\")\n",
    ")\n",
    "\n",
    "join_merge.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9196b111-1571-4b52-97cc-b2dad6d08321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파티션수 제어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d570c5d8-09fb-4d0e-8d04-b44c339aff1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "join_broadcast.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66034bd7-d56f-43e4-94da-223c50e9f250",
   "metadata": {},
   "outputs": [],
   "source": [
    "control = (\n",
    "    fact_df\n",
    "    .join(dim_df.hint(\"broadcast\"), \"user_id\")\n",
    "    .hint(\"repartition\", 8)\n",
    ")\n",
    "\n",
    "# control.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "410ff862-21c3-484c-bea4-61ec38eac649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "control.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fac6c0e2-c4f0-4e9c-b85a-43baab248954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hint가 무시되는 경우\n",
    "# 별칭을 안 맞춘 경우 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "805649b3-aaaa-4bf4-8555-9f2b4a7bec27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(5) SortMergeJoin [user_id#2L], [user_id#9L], Inner\n",
      ":- *(2) Sort [user_id#2L ASC NULLS FIRST], false, 0\n",
      ":  +- Exchange hashpartitioning(user_id#2L, 200), ENSURE_REQUIREMENTS, [plan_id=150]\n",
      ":     +- *(1) Project [id#0L AS user_id#2L, (rand(660007605823935748) * 100.0) AS amount#4]\n",
      ":        +- *(1) Range (0, 10000000, step=1, splits=10)\n",
      "+- *(4) Sort [user_id#9L ASC NULLS FIRST], false, 0\n",
      "   +- Exchange hashpartitioning(user_id#9L, 200), ENSURE_REQUIREMENTS, [plan_id=156]\n",
      "      +- *(3) Project [id#7L AS user_id#9L, US AS country#11]\n",
      "         +- *(3) Range (0, 100, step=1, splits=10)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/02 11:08:19 WARN HintErrorLogger: Count not find relation 'dim_table' specified in hint 'BROADCAST(dim_table)'.\n"
     ]
    }
   ],
   "source": [
    "# 1. SQL을 쓰기 위해 Temp View 등록\n",
    "fact_df.createOrReplaceTempView(\"fact_table\")\n",
    "dim_df.createOrReplaceTempView(\"dim_table\")\n",
    "\n",
    "# 상황: FROM dim_table AS d 라고 해놓고, 힌트에는 BROADCAST(dim_table)이라고 적음\n",
    "# 결과: 스파크는 'd'는 아는데 'dim_table'은 Alias에 가려져서 못 찾음 -> 힌트 무시!\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT /*+ BROADCAST(dim_table) */ * FROM fact_table f\n",
    "    JOIN dim_table d \n",
    "    ON f.user_id = d.user_id\n",
    "\"\"\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61ba63c7-b429-480e-8aed-66799af18565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# > SortMergeJoin [codegen id : ...] 힌트 무시됨 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f21a4b82-7575-4a89-86c3-5bd5536eb287",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cc3845-dac0-4777-98b8-750790e5f182",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
