{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ea9ebc3-87c3-4196-9e97-40d553193fdb",
   "metadata": {},
   "source": [
    "## Pyspark BroadCast\n",
    "\n",
    "1. python 객체 브로드캐스팅\n",
    "\n",
    "- 딕셔너리를 매핑할때 쓴다.\n",
    "- sparkContext를 통해 작은 참조데이터를 Executor(일꾼)들에게 전송\n",
    "\n",
    "occupation_dict = spark.sparkContext.broadcast(meta) -> 네트워크 전송은 딱 1번만 발생\n",
    "\n",
    "이때 occupation_dict는 데이터 그 자체가 아니라, Broadcast객체 -> .value를 써야한다\n",
    "\n",
    "F.udf : udf? User Defined Function 사용자 정의 함수\n",
    "스파크는 JVM 기반, 파이썬 코드를 직접 실행할수 없어서 이 컬럼 처리시 파이썬한테 물어보고 오라고 연결해주는 다리역할 \n",
    "\n",
    "2. Join\n",
    "\n",
    "- 참조 데이터가 딕셔너리가 아니라 또 다른 데이터 프레임 일때 쓴다\n",
    "\n",
    "\n",
    ">브로드캐스트는 데이터를 메모리에 통째로 올리는 방식\n",
    ">참조 데이터가 너무 크면 메모리가 터진다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc02fdf0-7b2d-4744-9afa-680255e2f9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import (\n",
    "    Row,\n",
    "    SparkSession)\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5698697-4829-4213-8d36-ecad01122c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/30 07:53:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark=(\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"broadcast_study\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6da30538-5ff5-41eb-9827-9cd372fc7baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|occupation_id|\n",
      "+-------------+\n",
      "|         1100|\n",
      "|         2030|\n",
      "|         1100|\n",
      "|         3801|\n",
      "|         9999|\n",
      "|         2030|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  (occupation_id만 있음)\n",
    "interviewer_count = spark.createDataFrame(\n",
    "    [\n",
    "        (\"1100\",),\n",
    "        (\"2030\",),\n",
    "        (\"1100\",),\n",
    "        (\"3801\",),\n",
    "        (\"9999\",),\n",
    "        (\"2030\",),\n",
    "    ],\n",
    "    [\"occupation_id\"]\n",
    ")\n",
    "\n",
    "interviewer_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00e86a72-5a01-40cb-9b25-1b27dcf85b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 작은 참조 데이터 (Python Dict)\n",
    "meta = {\n",
    "    \"1100\": \"engineer\",\n",
    "    \"2030\": \"developer\",\n",
    "    \"3801\": \"painter\",\n",
    "    \"3021\": \"chemistry teacher\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79f5f18f-e091-4359-809f-83f7b35a6431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 브로드캐스트 없이 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37569a73-4681-4ff4-a08a-e723adcd1e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function lookup_no_bc at 0xffff60e7ef70>\n"
     ]
    }
   ],
   "source": [
    "def lookup_no_bc(occupation_id):\n",
    "    return meta.get(occupation_id,\"Unknown\")\n",
    "\n",
    "lookup_no_bc_udf=F.udf(lookup_no_bc)\n",
    "\n",
    "print(lookup_no_bc_udf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8d7a74d-16dc-4897-8a8e-14d4d7b2bb58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+\n",
      "|occupation_id|occupation_name|\n",
      "+-------------+---------------+\n",
      "|         1100|       engineer|\n",
      "|         2030|      developer|\n",
      "|         1100|       engineer|\n",
      "|         3801|        painter|\n",
      "|         9999|        Unknown|\n",
      "|         2030|      developer|\n",
      "+-------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_no_bc=interviewer_count.withColumn(\n",
    "    \"occupation_name\",\n",
    "    lookup_no_bc_udf(F.col(\"occupation_id\"))\n",
    ")\n",
    "df_no_bc.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f49933b-1ae3-4f9d-94c8-c1163fcb87c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \t97 ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf757886-f7ea-4e15-9014-d203a9feb12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 브로드캐스트 사용ㅇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "185ff8ba-c826-4e4f-b1bd-9c546f4e0d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.broadcast.Broadcast object at 0xffff60e76fa0>\n",
      "{'1100': 'engineer', '2030': 'developer', '3801': 'painter', '3021': 'chemistry teacher'}\n"
     ]
    }
   ],
   "source": [
    "# 작은데이터를 모든 Excutor에게 전하기 \n",
    "# occupation_bc는 딕셔너리가 아니라 Broadcast객체!\n",
    "occupation_bc=spark.sparkContext.broadcast(meta)\n",
    "print(occupation_bc)\n",
    "print(occupation_bc.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e28e6648-fd0e-43e7-a52f-a42b80c5f1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_occupation_name(occupation_id:str)->str:\n",
    "    return occupation_bc.value.get(occupation_id,\"Unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7df2712-f7e4-41f6-8a8d-27fa18066a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# udf등록 \n",
    "occupation_lookup_udf=F.udf(get_occupation_name,StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc5a206b-ee30-4068-8ea4-265bb71b2322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+\n",
      "|occupation_id|occupation_name|\n",
      "+-------------+---------------+\n",
      "|         1100|       engineer|\n",
      "|         2030|      developer|\n",
      "|         1100|       engineer|\n",
      "|         3801|        painter|\n",
      "|         9999|        Unknown|\n",
      "|         2030|      developer|\n",
      "+-------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_result=interviewer_count.withColumn(\n",
    "    \"occupation_name\",\n",
    "    occupation_lookup_udf(F.col(\"occupation_id\"))\n",
    ")\n",
    "df_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9733e41-67f0-44a5-9e55-4f73b9f484c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \t81 ms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf720b4d-f923-48a2-86a9-43c65c2a45d6",
   "metadata": {},
   "source": [
    "### BroadCast 했을때랑 안했을때 시간 차이 \n",
    "\n",
    " Spark UI 에서 확인해 본 결과 \n",
    " \n",
    " BroadCast안했을때는  \t97 ms + 결과가 나올때 버벅거림 있다\n",
    " \n",
    " BroadCast했을때는 81ms + 결과가 빠르게 나오는 것을 확인했다\n",
    "\n",
    " >겨우 5개짜리 데이터에서도 이 정도 차이가 난다면, **실무 데이터(수만 건)** 에서는 **몇 분 vs 몇 시간**의 차이로 벌어질 수 있음!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfe93a8-6f6b-44e3-8d6a-2cb50c39661e",
   "metadata": {},
   "source": [
    "### Join패턴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85e46eee-946e-4b01-9a4a-670f07e854e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|user_id|   event|\n",
      "+-------+--------+\n",
      "|      1|   click|\n",
      "|      2|    view|\n",
      "|      3|   click|\n",
      "|      1|purchase|\n",
      "|      2|   click|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "big_df = spark.createDataFrame(\n",
    "    [\n",
    "        (1, \"click\"),\n",
    "        (2, \"view\"),\n",
    "        (3, \"click\"),\n",
    "        (1, \"purchase\"),\n",
    "        (2, \"click\"),\n",
    "    ],\n",
    "    [\"user_id\", \"event\"]\n",
    ")\n",
    "\n",
    "big_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f76b2eb-abf4-4f72-a31f-3f28c45e806b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|user_id|user_name|\n",
      "+-------+---------+\n",
      "|      1|    Alice|\n",
      "|      2|      Bob|\n",
      "|      3|  Charlie|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "small_df = spark.createDataFrame(\n",
    "    [\n",
    "        (1, \"Alice\"),\n",
    "        (2, \"Bob\"),\n",
    "        (3, \"Charlie\"),\n",
    "    ],\n",
    "    [\"user_id\", \"user_name\"]\n",
    ")\n",
    "\n",
    "small_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c662d529-16be-4e3a-9722-5e77cb48cb47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [user_id#35L, event#36, user_name#49]\n",
      "   +- SortMergeJoin [user_id#35L], [user_id#48L], Inner\n",
      "      :- Sort [user_id#35L ASC NULLS FIRST], false, 0\n",
      "      :  +- Exchange hashpartitioning(user_id#35L, 200), ENSURE_REQUIREMENTS, [plan_id=109]\n",
      "      :     +- Filter isnotnull(user_id#35L)\n",
      "      :        +- Scan ExistingRDD[user_id#35L,event#36]\n",
      "      +- Sort [user_id#48L ASC NULLS FIRST], false, 0\n",
      "         +- Exchange hashpartitioning(user_id#48L, 200), ENSURE_REQUIREMENTS, [plan_id=110]\n",
      "            +- Filter isnotnull(user_id#48L)\n",
      "               +- Scan ExistingRDD[user_id#48L,user_name#49]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_normal=big_df.join(small_df,\"user_id\")\n",
    "joined_normal.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d612a4c-b20f-452b-a37e-317afb228200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SortMergeJoin\n",
    "# Exchange -> 네트워크 데이터 이동 , 파티션을 다시 나누는 지점 \n",
    "# 셔플 발생"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce55f278-0617-4c00-a020-7f5592d05c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+---------+\n",
      "|user_id|   event|user_name|\n",
      "+-------+--------+---------+\n",
      "|      1|   click|    Alice|\n",
      "|      2|    view|      Bob|\n",
      "|      3|   click|  Charlie|\n",
      "|      1|purchase|    Alice|\n",
      "|      2|   click|      Bob|\n",
      "+-------+--------+---------+\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [user_id#35L, event#36, user_name#49]\n",
      "   +- BroadcastHashJoin [user_id#35L], [user_id#48L], Inner, BuildRight, false\n",
      "      :- Filter isnotnull(user_id#35L)\n",
      "      :  +- Scan ExistingRDD[user_id#35L,event#36]\n",
      "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=265]\n",
      "         +- Filter isnotnull(user_id#48L)\n",
      "            +- Scan ExistingRDD[user_id#48L,user_name#49]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Broadcast Join\n",
    "joined_bc=big_df.join(\n",
    "    F.broadcast(small_df),\n",
    "    \"user_id\"\n",
    ")\n",
    "joined_bc.show()\n",
    "joined_bc.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7d89563-9d1d-469e-b12a-bd6a02618212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BroadcastExchange HashedRelationBroadcastMode \n",
    "# BroadcastExchange 작은쪽데이터를 모든 Excutor복사, 셔플이 x\n",
    "# spakUI에서 SQL/DataFrame을 확인해보자 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb59b80-168b-47a5-9a48-49828c646117",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
