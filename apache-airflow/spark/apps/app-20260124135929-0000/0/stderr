Spark Executor Command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=33491" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@2ada9b9ccc3b:33491" "--executor-id" "0" "--hostname" "172.18.0.2" "--cores" "1" "--app-id" "app-20260124135929-0000" "--worker-url" "spark://Worker@172.18.0.2:33799" "--resourceProfileId" "0"
========================================

Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
26/01/24 13:59:29 INFO CoarseGrainedExecutorBackend: Started daemon with process name: 141@1787936f99ad
26/01/24 13:59:29 INFO SignalUtils: Registering signal handler for TERM
26/01/24 13:59:29 INFO SignalUtils: Registering signal handler for HUP
26/01/24 13:59:29 INFO SignalUtils: Registering signal handler for INT
26/01/24 13:59:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
26/01/24 13:59:30 INFO SecurityManager: Changing view acls to: spark
26/01/24 13:59:30 INFO SecurityManager: Changing modify acls to: spark
26/01/24 13:59:30 INFO SecurityManager: Changing view acls groups to: 
26/01/24 13:59:30 INFO SecurityManager: Changing modify acls groups to: 
26/01/24 13:59:30 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
26/01/24 13:59:30 INFO TransportClientFactory: Successfully created connection to 2ada9b9ccc3b/172.18.0.3:33491 after 42 ms (0 ms spent in bootstraps)
26/01/24 13:59:30 INFO SecurityManager: Changing view acls to: spark
26/01/24 13:59:30 INFO SecurityManager: Changing modify acls to: spark
26/01/24 13:59:30 INFO SecurityManager: Changing view acls groups to: 
26/01/24 13:59:30 INFO SecurityManager: Changing modify acls groups to: 
26/01/24 13:59:30 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
26/01/24 13:59:30 INFO TransportClientFactory: Successfully created connection to 2ada9b9ccc3b/172.18.0.3:33491 after 1 ms (0 ms spent in bootstraps)
26/01/24 13:59:30 INFO DiskBlockManager: Created local directory at /tmp/spark-2de42c56-f718-4088-9949-39b2c912391c/executor-bb518e60-cd0c-4365-9578-aabaaf3dc698/blockmgr-22cdb0aa-cac4-425c-9987-80b43d7d8ab8
26/01/24 13:59:30 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
26/01/24 13:59:30 INFO CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@2ada9b9ccc3b:33491
26/01/24 13:59:30 INFO WorkerWatcher: Connecting to worker spark://Worker@172.18.0.2:33799
26/01/24 13:59:30 INFO TransportClientFactory: Successfully created connection to /172.18.0.2:33799 after 1 ms (0 ms spent in bootstraps)
26/01/24 13:59:30 INFO WorkerWatcher: Successfully connected to spark://Worker@172.18.0.2:33799
26/01/24 13:59:30 INFO ResourceUtils: ==============================================================
26/01/24 13:59:30 INFO ResourceUtils: No custom resources configured for spark.executor.
26/01/24 13:59:30 INFO ResourceUtils: ==============================================================
26/01/24 13:59:30 INFO CoarseGrainedExecutorBackend: Successfully registered with driver
26/01/24 13:59:30 INFO Executor: Starting executor ID 0 on host 172.18.0.2
26/01/24 13:59:30 INFO Executor: OS info Linux, 6.12.54-linuxkit, aarch64
26/01/24 13:59:30 INFO Executor: Java version 17.0.13
26/01/24 13:59:30 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36551.
26/01/24 13:59:30 INFO NettyBlockTransferService: Server created on 172.18.0.2:36551
26/01/24 13:59:30 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
26/01/24 13:59:30 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(0, 172.18.0.2, 36551, None)
26/01/24 13:59:30 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(0, 172.18.0.2, 36551, None)
26/01/24 13:59:30 INFO BlockManager: Initialized BlockManager: BlockManagerId(0, 172.18.0.2, 36551, None)
26/01/24 13:59:30 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
26/01/24 13:59:30 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@6e70eaa5 for default.
26/01/24 13:59:30 INFO Executor: Fetching spark://2ada9b9ccc3b:33491/files/org.apache.hadoop_hadoop-aws-3.3.4.jar with timestamp 1769263168548
26/01/24 13:59:30 INFO TransportClientFactory: Successfully created connection to 2ada9b9ccc3b/172.18.0.3:33491 after 1 ms (0 ms spent in bootstraps)
26/01/24 13:59:30 INFO Utils: Fetching spark://2ada9b9ccc3b:33491/files/org.apache.hadoop_hadoop-aws-3.3.4.jar to /tmp/spark-2de42c56-f718-4088-9949-39b2c912391c/executor-bb518e60-cd0c-4365-9578-aabaaf3dc698/spark-f06a2531-72ad-448f-add9-a8765d5816b7/fetchFileTemp2820141734195264390.tmp
26/01/24 13:59:30 INFO Utils: Copying /tmp/spark-2de42c56-f718-4088-9949-39b2c912391c/executor-bb518e60-cd0c-4365-9578-aabaaf3dc698/spark-f06a2531-72ad-448f-add9-a8765d5816b7/64532171769263168548_cache to /opt/bitnami/spark/work/app-20260124135929-0000/0/./org.apache.hadoop_hadoop-aws-3.3.4.jar
26/01/24 13:59:30 INFO Executor: Fetching spark://2ada9b9ccc3b:33491/files/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar with timestamp 1769263168548
26/01/24 13:59:30 INFO Utils: Fetching spark://2ada9b9ccc3b:33491/files/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar to /tmp/spark-2de42c56-f718-4088-9949-39b2c912391c/executor-bb518e60-cd0c-4365-9578-aabaaf3dc698/spark-f06a2531-72ad-448f-add9-a8765d5816b7/fetchFileTemp6947870532551757972.tmp
26/01/24 13:59:30 INFO Utils: Copying /tmp/spark-2de42c56-f718-4088-9949-39b2c912391c/executor-bb518e60-cd0c-4365-9578-aabaaf3dc698/spark-f06a2531-72ad-448f-add9-a8765d5816b7/-17064334281769263168548_cache to /opt/bitnami/spark/work/app-20260124135929-0000/0/./org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar
26/01/24 13:59:30 INFO Executor: Fetching spark://2ada9b9ccc3b:33491/files/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar with timestamp 1769263168548
26/01/24 13:59:30 INFO Utils: Fetching spark://2ada9b9ccc3b:33491/files/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar to /tmp/spark-2de42c56-f718-4088-9949-39b2c912391c/executor-bb518e60-cd0c-4365-9578-aabaaf3dc698/spark-f06a2531-72ad-448f-add9-a8765d5816b7/fetchFileTemp4636504926342766801.tmp
26/01/24 13:59:31 INFO Utils: Copying /tmp/spark-2de42c56-f718-4088-9949-39b2c912391c/executor-bb518e60-cd0c-4365-9578-aabaaf3dc698/spark-f06a2531-72ad-448f-add9-a8765d5816b7/10044368371769263168548_cache to /opt/bitnami/spark/work/app-20260124135929-0000/0/./com.amazonaws_aws-java-sdk-bundle-1.12.262.jar
26/01/24 13:59:31 INFO Executor: Fetching spark://2ada9b9ccc3b:33491/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar with timestamp 1769263168548
26/01/24 13:59:31 INFO Utils: Fetching spark://2ada9b9ccc3b:33491/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar to /tmp/spark-2de42c56-f718-4088-9949-39b2c912391c/executor-bb518e60-cd0c-4365-9578-aabaaf3dc698/spark-f06a2531-72ad-448f-add9-a8765d5816b7/fetchFileTemp14541523471348437452.tmp
26/01/24 13:59:32 INFO Utils: /tmp/spark-2de42c56-f718-4088-9949-39b2c912391c/executor-bb518e60-cd0c-4365-9578-aabaaf3dc698/spark-f06a2531-72ad-448f-add9-a8765d5816b7/16338067521769263168548_cache has been previously copied to /opt/bitnami/spark/work/app-20260124135929-0000/0/./com.amazonaws_aws-java-sdk-bundle-1.12.262.jar
26/01/24 13:59:32 INFO Executor: Adding file:/opt/bitnami/spark/work/app-20260124135929-0000/0/./com.amazonaws_aws-java-sdk-bundle-1.12.262.jar to class loader default
26/01/24 13:59:32 INFO Executor: Fetching spark://2ada9b9ccc3b:33491/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar with timestamp 1769263168548
26/01/24 13:59:32 INFO Utils: Fetching spark://2ada9b9ccc3b:33491/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar to /tmp/spark-2de42c56-f718-4088-9949-39b2c912391c/executor-bb518e60-cd0c-4365-9578-aabaaf3dc698/spark-f06a2531-72ad-448f-add9-a8765d5816b7/fetchFileTemp11675671155342600921.tmp
26/01/24 13:59:32 INFO Utils: /tmp/spark-2de42c56-f718-4088-9949-39b2c912391c/executor-bb518e60-cd0c-4365-9578-aabaaf3dc698/spark-f06a2531-72ad-448f-add9-a8765d5816b7/5153682091769263168548_cache has been previously copied to /opt/bitnami/spark/work/app-20260124135929-0000/0/./org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar
26/01/24 13:59:32 INFO Executor: Adding file:/opt/bitnami/spark/work/app-20260124135929-0000/0/./org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar to class loader default
26/01/24 13:59:32 INFO Executor: Fetching spark://2ada9b9ccc3b:33491/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar with timestamp 1769263168548
26/01/24 13:59:32 INFO Utils: Fetching spark://2ada9b9ccc3b:33491/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar to /tmp/spark-2de42c56-f718-4088-9949-39b2c912391c/executor-bb518e60-cd0c-4365-9578-aabaaf3dc698/spark-f06a2531-72ad-448f-add9-a8765d5816b7/fetchFileTemp3866928195879565797.tmp
26/01/24 13:59:32 INFO Utils: /tmp/spark-2de42c56-f718-4088-9949-39b2c912391c/executor-bb518e60-cd0c-4365-9578-aabaaf3dc698/spark-f06a2531-72ad-448f-add9-a8765d5816b7/11560097561769263168548_cache has been previously copied to /opt/bitnami/spark/work/app-20260124135929-0000/0/./org.apache.hadoop_hadoop-aws-3.3.4.jar
26/01/24 13:59:32 INFO Executor: Adding file:/opt/bitnami/spark/work/app-20260124135929-0000/0/./org.apache.hadoop_hadoop-aws-3.3.4.jar to class loader default
26/01/24 13:59:32 INFO CoarseGrainedExecutorBackend: Got assigned task 0
26/01/24 13:59:32 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
26/01/24 13:59:32 INFO TorrentBroadcast: Started reading broadcast variable 1 with 1 pieces (estimated total size 4.0 MiB)
26/01/24 13:59:32 INFO TransportClientFactory: Successfully created connection to 2ada9b9ccc3b/172.18.0.3:39737 after 1 ms (0 ms spent in bootstraps)
26/01/24 13:59:32 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.5 KiB, free 434.4 MiB)
26/01/24 13:59:32 INFO TorrentBroadcast: Reading broadcast variable 1 took 51 ms
26/01/24 13:59:32 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.1 KiB, free 434.4 MiB)
26/01/24 13:59:32 INFO HadoopRDD: Input split: s3a://airflow-minio/input.txt:0+1501
26/01/24 13:59:32 INFO TorrentBroadcast: Started reading broadcast variable 0 with 1 pieces (estimated total size 4.0 MiB)
26/01/24 13:59:32 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 434.3 MiB)
26/01/24 13:59:32 INFO TorrentBroadcast: Reading broadcast variable 0 took 6 ms
26/01/24 13:59:32 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 367.5 KiB, free 434.0 MiB)
26/01/24 13:59:32 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
26/01/24 13:59:32 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
26/01/24 13:59:32 INFO MetricsSystemImpl: s3a-file-system metrics system started
26/01/24 13:59:33 INFO PythonRunner: Times: total = 443, boot = 382, init = 61, finish = 0
26/01/24 13:59:33 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1660 bytes result sent to driver
26/01/24 13:59:33 INFO CoarseGrainedExecutorBackend: Got assigned task 1
26/01/24 13:59:33 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
26/01/24 13:59:33 INFO HadoopRDD: Input split: s3a://airflow-minio/input.txt:1501+1502
26/01/24 13:59:34 INFO PythonRunner: Times: total = 42, boot = -165, init = 206, finish = 1
26/01/24 13:59:34 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1660 bytes result sent to driver
26/01/24 13:59:34 INFO CoarseGrainedExecutorBackend: Got assigned task 2
26/01/24 13:59:34 INFO Executor: Running task 0.0 in stage 1.0 (TID 2)
26/01/24 13:59:34 INFO MapOutputTrackerWorker: Updating epoch to 1 and clearing cache
26/01/24 13:59:34 INFO TorrentBroadcast: Started reading broadcast variable 2 with 1 pieces (estimated total size 4.0 MiB)
26/01/24 13:59:34 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.9 KiB, free 434.0 MiB)
26/01/24 13:59:34 INFO TorrentBroadcast: Reading broadcast variable 2 took 6 ms
26/01/24 13:59:34 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 11.7 KiB, free 434.0 MiB)
26/01/24 13:59:34 INFO MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
26/01/24 13:59:34 INFO MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@2ada9b9ccc3b:33491)
26/01/24 13:59:34 INFO MapOutputTrackerWorker: Got the map output locations
26/01/24 13:59:34 INFO ShuffleBlockFetcherIterator: Getting 2 (1744.0 B) non-empty blocks including 2 (1744.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/01/24 13:59:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
26/01/24 13:59:34 INFO PythonRunner: Times: total = 43, boot = -97, init = 139, finish = 1
26/01/24 13:59:34 INFO Executor: Finished task 0.0 in stage 1.0 (TID 2). 2150 bytes result sent to driver
26/01/24 13:59:35 INFO CoarseGrainedExecutorBackend: Got assigned task 3
26/01/24 13:59:35 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
26/01/24 13:59:35 INFO TorrentBroadcast: Started reading broadcast variable 3 with 1 pieces (estimated total size 4.0 MiB)
26/01/24 13:59:35 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 84.7 KiB, free 433.9 MiB)
26/01/24 13:59:35 INFO TorrentBroadcast: Reading broadcast variable 3 took 6 ms
26/01/24 13:59:35 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 227.8 KiB, free 433.7 MiB)
26/01/24 13:59:35 INFO ShuffleBlockFetcherIterator: Getting 2 (1744.0 B) non-empty blocks including 2 (1744.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/01/24 13:59:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/01/24 13:59:35 INFO CodeGenerator: Code generated in 100.268542 ms
26/01/24 13:59:35 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.
26/01/24 13:59:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
26/01/24 13:59:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
26/01/24 13:59:35 INFO AbstractS3ACommitterFactory: Using Committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202601241359356792358124799358235_0003}; taskId=attempt_202601241359356792358124799358235_0003_m_000000_3, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@656f05d4}; outputPath=s3a://airflow-minio/output, workPath=s3a://airflow-minio/output/_temporary/0/_temporary/attempt_202601241359356792358124799358235_0003_m_000000_3, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false} for s3a://airflow-minio/output
26/01/24 13:59:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
26/01/24 13:59:35 INFO PythonRunner: Times: total = 43, boot = -1373, init = 1415, finish = 1
26/01/24 13:59:35 INFO FileOutputCommitter: Saved output of task 'attempt_202601241359356792358124799358235_0003_m_000000_3' to s3a://airflow-minio/output/_temporary/0/task_202601241359356792358124799358235_0003_m_000000
26/01/24 13:59:35 INFO SparkHadoopMapRedUtil: attempt_202601241359356792358124799358235_0003_m_000000_3: Committed. Elapsed time: 81 ms.
26/01/24 13:59:35 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 3692 bytes result sent to driver
26/01/24 13:59:35 INFO CoarseGrainedExecutorBackend: Got assigned task 4
26/01/24 13:59:35 INFO Executor: Running task 1.0 in stage 3.0 (TID 4)
26/01/24 13:59:35 INFO ShuffleBlockFetcherIterator: Getting 2 (1657.0 B) non-empty blocks including 2 (1657.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
26/01/24 13:59:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
26/01/24 13:59:35 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.
26/01/24 13:59:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
26/01/24 13:59:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
26/01/24 13:59:35 INFO AbstractS3ACommitterFactory: Using Committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202601241359356792358124799358235_0003}; taskId=attempt_202601241359356792358124799358235_0003_m_000001_4, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7d795a04}; outputPath=s3a://airflow-minio/output, workPath=s3a://airflow-minio/output/_temporary/0/_temporary/attempt_202601241359356792358124799358235_0003_m_000001_4, algorithmVersion=1, skipCleanup=false, ignoreCleanupFailures=false} for s3a://airflow-minio/output
26/01/24 13:59:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
26/01/24 13:59:36 INFO PythonRunner: Times: total = 43, boot = -366, init = 408, finish = 1
26/01/24 13:59:36 INFO FileOutputCommitter: Saved output of task 'attempt_202601241359356792358124799358235_0003_m_000001_4' to s3a://airflow-minio/output/_temporary/0/task_202601241359356792358124799358235_0003_m_000001
26/01/24 13:59:36 INFO SparkHadoopMapRedUtil: attempt_202601241359356792358124799358235_0003_m_000001_4: Committed. Elapsed time: 54 ms.
26/01/24 13:59:36 INFO Executor: Finished task 1.0 in stage 3.0 (TID 4). 3606 bytes result sent to driver
26/01/24 13:59:36 INFO CoarseGrainedExecutorBackend: Driver commanded a shutdown
26/01/24 13:59:36 INFO MemoryStore: MemoryStore cleared
26/01/24 13:59:36 INFO BlockManager: BlockManager stopped
26/01/24 13:59:36 INFO ShutdownHookManager: Shutdown hook called
26/01/24 13:59:36 INFO ShutdownHookManager: Deleting directory /tmp/spark-2de42c56-f718-4088-9949-39b2c912391c/executor-bb518e60-cd0c-4365-9578-aabaaf3dc698/spark-f06a2531-72ad-448f-add9-a8765d5816b7
26/01/24 13:59:36 ERROR CoarseGrainedExecutorBackend: RECEIVED SIGNAL TERM
de42c56-f718-4088-9949-39b2c912391c/executor-bb518e60-cd0c-4365-9578-aabaaf3dc698/spark-f06a2531-72ad-448f-add9-a8765d5816b7
