from airflow.decorators import dag
from airflow.providers.amazon.aws.sensors.s3 import S3KeySensor
from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator
import pendulum

# ê¸°ë³¸ ì„¤ì •
default_args = {
    "owner": "airflow",
    'start_date': pendulum.datetime(2026, 1, 20, tz="Asia/Seoul"),
    "retries": 1,
}

@dag(
    default_args=default_args,
    schedule=None,      # ìˆ˜ë™ ì‹¤í–‰ (ì›í•˜ë©´ '@daily' ë“±ìœ¼ë¡œ ë³€ê²½)
    catchup=False,
    tags=['spark', 'minio'],
)
def spark_word_count_taskflow():

    # 1. ê°ì‹œì: MinIOì— íŒŒì¼ì´ ìˆë‚˜ í™•ì¸ (Sensor)
    check_minio_file = S3KeySensor(
        task_id='check_minio_file',
        bucket_name='airflow-minio',       # â­ï¸ ìš°ë¦¬ ë²„í‚· ì´ë¦„
        bucket_key='input.txt',            # ê°ì‹œí•  íŒŒì¼ëª…
        aws_conn_id='minio_default',       # ì•„ê¹Œ ë§Œë“  ì—°ê²° ID
        poke_interval=10,                  # 10ì´ˆë§ˆë‹¤ í™•ì¸
        timeout=60 * 5                     # 5ë¶„ ë™ì•ˆ ì•ˆ ì˜¤ë©´ í¬ê¸°
    )

    # 2. ì‹¤í–‰ì: Sparkì—ê²Œ ì¼ ì‹œí‚¤ê¸° (Operator)
    spark_task = SparkSubmitOperator(
task_id="spark_submit_task",
        conn_id="spark_default",
        application='/opt/airflow/dags/word_count_app.py',
        
        # ğŸ‘‡ [í•µì‹¬ ìˆ˜ì •] ì„¤ì •ì„ ì—¬ê¸°ì„œ ê°•ì œë¡œ ì£¼ì…í•©ë‹ˆë‹¤!
        packages="org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262",
        conf={
            # 1. ëŒì•„ì˜¤ëŠ” ê¸¸(Callback) ì„¤ì •
            "spark.driver.bindAddress": "0.0.0.0",
            "spark.driver.host": "host.docker.internal",  # Macì—ì„œëŠ” ì´ê²Œ ì •ë‹µì…ë‹ˆë‹¤
            "spark.driver.port": "20000",
            "spark.blockManager.port": "20001",
            
            # 2. MinIO ì„¤ì • (DAGì—ì„œ ê°•ì œ ì§€ì •í•˜ë©´ í™•ì‹¤í•¨)
            "spark.hadoop.fs.s3a.endpoint": "http://minio:9000",
            "spark.hadoop.fs.s3a.access.key": "ROOTNAME",
            "spark.hadoop.fs.s3a.secret.key": "CHANGEME123",
            "spark.hadoop.fs.s3a.path.style.access": "true",
            "spark.hadoop.fs.s3a.impl": "org.apache.hadoop.fs.s3a.S3AFileSystem",
            "spark.hadoop.fs.connection.ssl.enabled": "false",
        },
        
        # ğŸ‘‡ [í•µì‹¬ ìˆ˜ì •] Java ìœ„ì¹˜ë¥¼ ê°•ì œë¡œ ì•Œë ¤ì¤ë‹ˆë‹¤.
        env_vars={"JAVA_HOME": "/usr/lib/jvm/java-17-openjdk-arm64"},
        
        application_args=[
            "s3a://airflow-minio/input.txt", 
            "s3a://airflow-minio/output_airflow"
        ]
    )

    # 3. ìˆœì„œ ì—°ê²° (Sensorê°€ ì„±ê³µí•˜ë©´ -> Spark ì‹¤í–‰)
    check_minio_file >> spark_task

# DAG ìƒì„±
spark_dag = spark_word_count_taskflow()