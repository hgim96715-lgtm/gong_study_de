import sys
from pyspark.sql import SparkSession

# ğŸ‘‡ ì—¬ê¸°ì— Airflow ê´€ë ¨ ì½”ë“œëŠ” í•œ ì¤„ë„ ì—†ì–´ì•¼ í•©ë‹ˆë‹¤!
if __name__ == "__main__":
    spark = SparkSession.builder.appName("WordCountApp").getOrCreate()

    # ì¸ì í™•ì¸
    if len(sys.argv) < 3:
        sys.exit("Usage: word_count_app.py <input> <output>")

    input_path = sys.argv[1]
    output_path = sys.argv[2]

    print(f"DEBUG: Reading {input_path} -> Writing to {output_path}")

    try:
        # ì½ê¸°
        df = spark.read.text(input_path)
        # ë¡œì§ (ë‹¨ì–´ ì„¸ê¸°)
        counts = df.rdd.flatMap(lambda x: x.value.split(" ")) \
                       .map(lambda x: (x, 1)) \
                       .reduceByKey(lambda a, b: a + b)
        # ì“°ê¸°
        counts.saveAsTextFile(output_path)
        print("DEBUG: Success!")
    except Exception as e:
        print(f"DEBUG: Error: {e}")
        raise e
    finally:
        spark.stop()